import uuid,os
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.messages import AIMessage,ToolMessage
# from ollama_deep_researcher.tools import llm_calculator_tool,generate_xdl_protocol
from src.ollama_deep_researcher.tools import query_edge_server,dispatch_task_and_monitor
from src.agent_xdl.tools1 import llm_calculator_tool,generate_xdl_protocol,weather_tool
# from langgraph.checkpoint.sqlite import SqliteSaver

# export PYTHONPATH=/home/pfjial/local-deep-researcher-main

tools = [llm_calculator_tool,generate_xdl_protocol,query_edge_server,dispatch_task_and_monitor,weather_tool]
tool_node = ToolNode(tools)

def should_continue(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    if getattr(last_message, "tool_calls", None):
        return "tools"
    return END
# 1764728646727FBys1MQS2i7TX48XcRbrLxg
model_with_tools = ChatOpenAI(
        model=os.getenv("XDL_LLM_MODEL"),
        api_key=os.getenv("XDL_LLM_API_KEY"),
        openai_api_base=os.getenv("XDL_LLM_API_BASE"),
        temperature=0.1
    ).bind_tools(tools=tools)

def call_model(state: MessagesState) -> MessagesState:
    messages = state["messages"]
    
    # ğŸ”´ æ ¸å¿ƒåˆ¤æ–­ï¼šæ˜¯å¦å·²ç»æ‰§è¡Œå®Œæ‰€æœ‰å·¥å…·ä¸”ç»“æœç¬¦åˆè¦æ±‚
    # 1. æ‰¾åˆ°æœ€åä¸€æ¡å·¥å…·æ‰§è¡Œç»“æœï¼ˆToolMessageï¼‰
    tool_messages = [msg for msg in messages if isinstance(msg, ToolMessage)]
    # 2. æ‰¾åˆ°æœ€åä¸€æ¡Agentæ¶ˆæ¯ï¼ˆåˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ–°å·¥å…·è°ƒç”¨ï¼‰
    ai_messages = [msg for msg in messages if isinstance(msg, AIMessage)]
    
    # ç»ˆæ­¢æ¡ä»¶ï¼šæœ‰å·¥å…·æ‰§è¡Œç»“æœ + æœ€åä¸€æ¡AIæ¶ˆæ¯æ— æ–°å·¥å…·è°ƒç”¨ â†’ ç›´æ¥è¿”å›ï¼Œè·³è¿‡LLMæ•´ç†
    if tool_messages and ai_messages:
        last_ai_msg = ai_messages[-1]
        # æ£€æŸ¥æœ€åä¸€æ¡AIæ¶ˆæ¯æ˜¯å¦æœ‰æœªæ‰§è¡Œçš„tool_calls
        if not getattr(last_ai_msg, "tool_calls", None):
            # æ— æ–°å·¥å…·è°ƒç”¨ â†’ ç›´æ¥è¿”å›å½“å‰stateï¼Œä¸è°ƒç”¨LLM
            return state
    
    # ğŸ”´ ä»…å½“éœ€è¦ç»§ç»­å¤„ç†æ—¶ï¼Œæ‰è°ƒç”¨LLM
    response = model_with_tools.invoke(messages)

    # è‡ªåŠ¨è¡¥ä¸Šç¼ºå¤±çš„ tool_call_id
    if isinstance(response, AIMessage) and getattr(response, "tool_calls", None):
        for tool_call in response.tool_calls:
            if not tool_call.get("id"):
                tool_call["id"] = f"call_{uuid.uuid4().hex[:8]}"

    state['messages'].append(response)
    return state

# checkpointer = SqliteSaver.from_conn_string(
#     "file:./langgraph_chat.db?mode=rwc"
# )

graph = StateGraph(MessagesState)
graph.add_node("agent", call_model)
graph.add_node("tools", tool_node)
graph.add_edge(START, "agent")
graph.add_conditional_edges("agent", should_continue, ["tools", END])
graph.add_edge("tools", "agent")
app = graph.compile()
# app = graph.compile(checkpointer=checkpointer)


if __name__ == "__main__":
    # res = app.invoke(
    #     {"messages": [{"role": "user", "content": "è®¡ç®—ä¸‹897*678"}]},
    #     thread_id="thread-1"
    # )
    # print(res)
    print("===="*20)
    for chunk in app.stream(
            {"messages": [{"role": "user", "content": "åˆæˆæ°§åŒ–é”†çš„æ··åˆå‰é©±ä½“é˜¶æ®µï¼Œç”Ÿæˆå®éªŒæ­¥éª¤ä¸­çš„æ ¸å¿ƒåŠ¨ä½œä»¥æ··åˆä¸ºä¸»çš„xdl"}]},
            stream_mode="updates"):
        pass
        # print(chunk)

    # print(f"SQLite DB å†™å…¥è·¯å¾„ï¼š./langgraph_chat.db")