# langgraph_server.py
from fastapi import FastAPI
import uvicorn
import requests
import asyncio
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Dict, Any

# -----------------------------
# 1. å®šä¹‰çŠ¶æ€ç»“æ„
# -----------------------------
class WorkflowState(TypedDict):
    steps: List[Dict[str, Any]]
    current: int
    results: List[str]

# -----------------------------
# 2. å®šä¹‰æ‰§è¡ŒèŠ‚ç‚¹
# -----------------------------
async def dispatch_step(state: WorkflowState):
    idx = state["current"]
    step = state["steps"][idx]

    print(f"â¡ Dispatching step {idx+1}: {step}")

    # è°ƒç”¨ edge server
    resp = requests.post(
        "http://127.0.0.1:9000/device/execute",
        json={"device": "ELx405_01", "step": step},
    )
    data = resp.json()

    # æ›´æ–°ç»“æœ
    new_results = state["results"] + [data["step"]]

    return {
        "results": new_results,
        "current": idx + 1,
    }

# -----------------------------
# 3. æ„å»º LangGraph workflow
# -----------------------------
checkpointer = MemorySaver()
builder = StateGraph(WorkflowState)

builder.add_node("dispatch_step", dispatch_step)
builder.set_entry_point("dispatch_step")

# step å¾ªç¯
builder.add_conditional_edges(
    "dispatch_step",
    lambda s: "dispatch_step" if s["current"] < len(s["steps"]) else END,
    {
        "dispatch_step": "dispatch_step",
        END: END,
    }
)

graph = builder.compile()

# -----------------------------
# 4. FastAPI æš´éœ² /run
# -----------------------------
app = FastAPI()

@app.post("/run")
async def run_workflow():
    print("ğŸš€ Starting workflow...")

    initial_state = {
        "steps": [
            {"step": "prime"},
            {"step": "wash"},
            {"step": "read_signal"},
        ],
        "current": 0,
        "results": [],
    }

    # ğŸ”¥ LangGraph è¦æ±‚çš„æ­£ç¡® config
    config = {"configurable": {"thread_id": "xyz"}}

    final_state = None

    # ğŸ”¥ ä¿®å¤ï¼šå¿…é¡»å¸¦ configï¼Œå¿…é¡»å¸¦ async streaming
    async for event in graph.astream(initial_state, config):
        final_state = event

    return {"workflow_result": final_state}


if __name__ == "__main__":
    print("ğŸš€ LangGraph server at 8000 ...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
